{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "from sklearn import preprocessing\n",
    "import sys\n",
    "import h5py\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "        \n",
    "\n",
    "\n",
    "class DATA_LOADER(object):\n",
    "    def __init__(self, opt):\n",
    "        self.read_dataset(opt)\n",
    "        self.index_in_epoch = 0\n",
    "        self.epochs_completed = 0\n",
    "\n",
    "    def read_dataset(self, opt):\n",
    "        file_path = opt.dataroot + \"/\"\n",
    "\n",
    "        self.train_feature = np.load(file_path+\"features/attention_feature_train_512.npy\") #(8000,512)\n",
    "        self.train_label = np.load(file_path+\"train_origin/y_train.npy\").reshape(-1)       #(1,8000*2048)\n",
    "        self.test_unseen_feature = np.load(file_path+\"features/attention_feature_test_512.npy\")\n",
    "        self.test_unseen_label = np.load(file_path+\"test_origin/y_test.npy\").reshape(-1)\n",
    "        self.attribute = np.load(file_path+\"semantic/semantic_all_28_2.npy\")               #(8,28)\n",
    "#         self.attribute = np.load(file_path+\"semantic/semantic_all_fre_tim_ae.npy\")\n",
    "\n",
    "\n",
    "        if opt.preprocessing:\n",
    "            if opt.standardization:\n",
    "                print('standardization...')\n",
    "                scaler = preprocessing.StandardScaler()\n",
    "            else:\n",
    "                scaler = preprocessing.MinMaxScaler() \n",
    "                \n",
    "        self.train_feature = scaler.fit_transform(self.train_feature) \n",
    "        \n",
    "        self.test_unseen_feature = scaler.fit_transform(self.test_unseen_feature)\n",
    "        \n",
    "    #         self.test_unseen_feature = torch.from_numpy(feature[val_unseen_loc]).float()\n",
    "    #         self.test_unseen_label = torch.from_numpy(label[val_unseen_loc]).long()\n",
    "        \n",
    "    \n",
    "        self.train_feature = torch.from_numpy(self.train_feature).float()\n",
    "#         mx = self.train_feature.max()\n",
    "#         self.train_feature.mul_(1 / mx)\n",
    "        self.train_label = torch.from_numpy(self.train_label).long() \n",
    "        \n",
    "        self.test_seen_feature = self.train_feature # gzsl的test_seen\n",
    "        self.test_seen_label = self.train_label\n",
    "        \n",
    "        self.test_unseen_feature = torch.from_numpy(self.test_unseen_feature).float()\n",
    "#         mx = self.test_unseen_feature.max()\n",
    "#         self.test_unseen_feature.mul_(1 / mx)\n",
    "        self.test_unseen_label = torch.from_numpy(self.test_unseen_label).long()\n",
    "        self.attribute = torch.from_numpy(self.attribute).float()\n",
    "        \n",
    "        self.seenclasses = torch.from_numpy(np.unique(self.train_label))\n",
    "        self.unseenclasses = torch.from_numpy(np.unique(self.test_unseen_label))\n",
    "        print(self.seenclasses)\n",
    "        print(self.unseenclasses)\n",
    "\n",
    "#         self.ntrain = self.train_feature.shape[0]\n",
    "\n",
    "        self.ntrain = self.train_feature.size()[0]\n",
    "        self.ntrain_class = self.seenclasses.size(0)\n",
    "        self.ntest_class = self.unseenclasses.size(0)\n",
    "        self.train_class = self.seenclasses.clone()\n",
    "        self.allclasses = torch.arange(0, self.ntrain_class + self.ntest_class).long() \n",
    "        print(self.allclasses)\n",
    "        self.attribute_seen = self.attribute[self.seenclasses]\n",
    "\n",
    "        # collect the data of each class\n",
    "\n",
    "        self.train_samples_class_index = torch.tensor([self.train_label.eq(i_class).sum().float() for i_class in self.train_class])\n",
    "\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        idx = torch.randperm(self.ntrain)[0:batch_size] \n",
    "        batch_feature = self.train_feature[idx]\n",
    "        batch_label = self.train_label[idx]\n",
    "        batch_att = self.attribute[batch_label].squeeze()\n",
    "        return batch_feature, batch_label, batch_att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SupConLoss_clear(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super(SupConLoss_clear, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, features, labels):\n",
    "\n",
    "#         device = (torch.device('cuda')\n",
    "#                   if features.is_cuda\n",
    "#                   else torch.device('cpu'))\n",
    "\n",
    "        batch_size = features.shape[0]\n",
    "        '''\n",
    "        示例: \n",
    "        labels: \n",
    "            tensor([[1.],\n",
    "                    [2.],\n",
    "                    [1.],\n",
    "                    [1.]])\n",
    "        mask:  # 两个样本i,j的label相等时，mask_{i,j}=1\n",
    "            tensor([[1., 0., 1., 1.],\n",
    "                    [0., 1., 0., 0.],\n",
    "                    [1., 0., 1., 1.],\n",
    "                    [1., 0., 1., 1.]]) \n",
    "        '''\n",
    "        labels = labels.contiguous().view(-1, 1)\n",
    "        mask = torch.eq(labels, labels.T).float() \n",
    "#         mask = torch.eq(labels, labels.T).float().to(device)\n",
    "\n",
    "        anchor_dot_contrast = torch.div(\n",
    "            torch.matmul(features, features.T),\n",
    "            self.temperature)\n",
    "\n",
    "        # normalize the logits for numerical stability\n",
    "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "        logits = anchor_dot_contrast - logits_max.detach()\n",
    "        exp_logits = torch.exp(logits)\n",
    "        '''\n",
    "        logits是anchor_dot_contrast减去每一行的最大值得到的最终相似度\n",
    "        示例: logits: torch.size([4,4])\n",
    "        logits:\n",
    "            tensor([[ 0.0000, -0.0471, -0.3352, -0.2156],\n",
    "                    [-1.2576,  0.0000, -0.3367, -0.0725],\n",
    "                    [-1.3500, -0.1409, -0.1420,  0.0000],\n",
    "                    [-1.4312, -0.0776, -0.2009,  0.0000]])       \n",
    "        '''\n",
    "        # 构建mask \n",
    "        logits_mask = torch.ones_like(mask) - torch.eye(batch_size)     \n",
    "        positives_mask = mask * logits_mask\n",
    "        negatives_mask = 1. - mask \n",
    "        '''\n",
    "        但是对于计算Loss而言，(i,i)位置表示样本本身的相似度，对Loss是没用的，所以要mask掉\n",
    "        # 第ind行第ind位置填充为0\n",
    "        得到logits_mask:\n",
    "            tensor([[0., 1., 1., 1.],\n",
    "                    [1., 0., 1., 1.],\n",
    "                    [1., 1., 0., 1.],\n",
    "                    [1., 1., 1., 0.]])\n",
    "        positives_mask:\n",
    "        tensor([[0., 0., 1., 1.],\n",
    "                [0., 0., 0., 0.],\n",
    "                [1., 0., 0., 1.],\n",
    "                [1., 0., 1., 0.]])\n",
    "        negatives_mask:\n",
    "        tensor([[0., 1., 0., 0.],\n",
    "                [1., 0., 1., 1.],\n",
    "                [0., 1., 0., 0.],\n",
    "                [0., 1., 0., 0.]])\n",
    "        '''        \n",
    "        num_positives_per_row  = torch.sum(positives_mask , axis=1) # 除了自己之外，正样本的个数  [2 0 2 2]       \n",
    "        denominator = torch.sum(\n",
    "        exp_logits * negatives_mask, axis=1, keepdims=True) + torch.sum(\n",
    "            exp_logits * positives_mask, axis=1, keepdims=True)  \n",
    "        \n",
    "        log_probs = logits - torch.log(denominator)\n",
    "        if torch.any(torch.isnan(log_probs)):\n",
    "            raise ValueError(\"Log_prob has nan!\")\n",
    "        \n",
    "\n",
    "        log_probs = torch.sum(\n",
    "            log_probs*positives_mask , axis=1)[num_positives_per_row > 0] / num_positives_per_row[num_positives_per_row > 0]\n",
    "        '''\n",
    "        计算正样本平均的log-likelihood\n",
    "        考虑到一个类别可能只有一个样本，就没有正样本了 比如我们labels的第二个类别 labels[1,2,1,1]\n",
    "        所以这里只计算正样本个数>0的    \n",
    "        '''\n",
    "        # loss\n",
    "        loss = -log_probs\n",
    "        loss *= self.temperature\n",
    "        loss = loss.mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LINEAR_LOGSOFTMAX1(nn.Module):\n",
    "    def __init__(self, input_dim, nclass):\n",
    "        super(LINEAR_LOGSOFTMAX1, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, nclass)\n",
    "        self.logic = nn.LogSoftmax(dim=1)\n",
    "    def forward(self, x): \n",
    "        o = self.logic(self.fc(x)) \n",
    "        return o \n",
    "    \n",
    "class CLASSIFIER_TRAIN:\n",
    "    # train_Y is interger \n",
    "    def __init__(self, _train_X, _train_Y, map_net, resSize, data_loader, _nclass, _cuda, _lr=0.00001, _beta1=0.5, _nepoch=20, _batch_size=100, generalized=True):\n",
    "        self.train_X =  _train_X \n",
    "        self.train_Y = _train_Y \n",
    "        \n",
    "        self.test_seen_feature = data_loader.test_seen_feature \n",
    "        self.test_seen_label = data_loader.test_seen_label \n",
    "        \n",
    "        self.test_unseen_feature = data_loader.test_unseen_feature \n",
    "        self.test_unseen_label = data_loader.test_unseen_label \n",
    "        \n",
    "        self.seenclasses = data_loader.seenclasses\n",
    "        self.unseenclasses = data_loader.unseenclasses\n",
    "        \n",
    "        self.MapNet=map_net #\n",
    "        \n",
    "        self.batch_size = _batch_size\n",
    "        self.nepoch = _nepoch\n",
    "        \n",
    "        self.nclass = _nclass\n",
    "        self.input_dim = resSize\n",
    "        self.cuda = _cuda\n",
    "        self.model =  LINEAR_LOGSOFTMAX1(self.input_dim, self.nclass)\n",
    "\n",
    "        self.model.apply(weights_init)\n",
    "        self.criterion = nn.NLLLoss() \n",
    "        \n",
    "        self.input = torch.FloatTensor(_batch_size, _train_X.size(1))\n",
    "        self.label = torch.LongTensor(_batch_size) \n",
    "        \n",
    "        self.lr = _lr\n",
    "        self.beta1 = _beta1\n",
    "        # setup optimizer\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=_lr, betas=(_beta1, 0.999))\n",
    "\n",
    "        # if self.cuda:\n",
    "            # self.model.cuda()\n",
    "            # self.criterion.cuda()\n",
    "            # self.input = self.input.cuda()\n",
    "            # self.label = self.label.cuda()\n",
    "\n",
    "        self.index_in_epoch = 0\n",
    "        self.epochs_completed = 0\n",
    "        self.ntrain = self.train_X.size()[0]\n",
    "\n",
    "        if generalized:\n",
    "            self.acc_seen, self.acc_unseen, self.H = self.fit()\n",
    "        else:\n",
    "            self.acc = self.fit_zsl()\n",
    "    \n",
    "    \n",
    "    def fit_zsl(self):\n",
    "        best_acc = 0\n",
    "        mean_loss = 0\n",
    "        for epoch in range(self.nepoch):\n",
    "            for i in range(0, self.ntrain, self.batch_size):      \n",
    "                self.model.zero_grad()\n",
    "                batch_input, batch_label = self.next_batch(self.batch_size)\n",
    "                self.input.copy_(batch_input)\n",
    "                self.label.copy_(batch_label)  \n",
    "#                 embed, _=self.MapNet(self.input)\n",
    "#                 output = self.model(embed)\n",
    "                output = self.model(self.input)\n",
    "                loss = self.criterion(output, self.label)\n",
    "                mean_loss += loss.data\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            acc = self.val(self.test_seen_feature, self.test_seen_label, self.seenclasses)\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "        print('Training classifier_train loss= %.4f' % (loss))\n",
    "        return best_acc \n",
    "\n",
    "    def fit(self):\n",
    "        best_H = 0\n",
    "        best_seen = 0\n",
    "        best_unseen = 0\n",
    "        for epoch in range(self.nepoch):\n",
    "            for i in range(0, self.ntrain, self.batch_size):      \n",
    "                self.model.zero_grad()\n",
    "                batch_input, batch_label = self.next_batch(self.batch_size) \n",
    "                self.input.copy_(batch_input)\n",
    "                self.label.copy_(batch_label)\n",
    "\n",
    "#                 embed, _ = self.MapNet(self.input)\n",
    "#                 output = self.model(embed)\n",
    "                output = self.model(self.input)\n",
    "                loss = self.criterion(output, self.label)\n",
    "\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            acc_seen = self.val_gzsl(self.test_seen_feature, self.test_seen_label, self.seenclasses)\n",
    "            acc_unseen = self.val_gzsl(self.test_unseen_feature, self.test_unseen_label, self.unseenclasses)\n",
    "            if (acc_seen+acc_unseen)==0:  \n",
    "                print('a bug')\n",
    "                H=0\n",
    "            else:\n",
    "                H = 2*acc_seen*acc_unseen / (acc_seen+acc_unseen)\n",
    "            if H > best_H:\n",
    "                best_seen = acc_seen\n",
    "                best_unseen = acc_unseen\n",
    "                best_H = H\n",
    "        return best_seen, best_unseen, best_H\n",
    "                     \n",
    "    def next_batch(self, batch_size): #\n",
    "        start = self.index_in_epoch\n",
    "        # shuffle the data at the first epoch 随机\n",
    "        if self.epochs_completed == 0 and start == 0: \n",
    "            perm = torch.randperm(self.ntrain) \n",
    "            self.train_X = self.train_X[perm]\n",
    "            self.train_Y = self.train_Y[perm]\n",
    "        if start + batch_size > self.ntrain:\n",
    "            self.epochs_completed += 1\n",
    "            rest_num_examples = self.ntrain - start\n",
    "            if rest_num_examples > 0:\n",
    "                X_rest_part = self.train_X[start:self.ntrain]\n",
    "                Y_rest_part = self.train_Y[start:self.ntrain]\n",
    "            # shuffle the data\n",
    "            perm = torch.randperm(self.ntrain)\n",
    "            self.train_X = self.train_X[perm]\n",
    "            self.train_Y = self.train_Y[perm]\n",
    "            # start next epoch ，last part\n",
    "            start = 0\n",
    "            self.index_in_epoch = batch_size - rest_num_examples\n",
    "            end = self.index_in_epoch\n",
    "            X_new_part = self.train_X[start:end]\n",
    "            Y_new_part = self.train_Y[start:end]\n",
    "            #print(start, end)\n",
    "            if rest_num_examples > 0:\n",
    "                return torch.cat((X_rest_part, X_new_part), 0) , torch.cat((Y_rest_part, Y_new_part), 0)\n",
    "            else:\n",
    "                return X_new_part, Y_new_part\n",
    "        else:\n",
    "            self.index_in_epoch += batch_size\n",
    "            end = self.index_in_epoch\n",
    "            #print(start, end)\n",
    "            # from index start to index end-1\n",
    "            return self.train_X[start:end], self.train_Y[start:end]\n",
    "\n",
    "\n",
    "    def val_gzsl(self, test_X, test_label, target_classes): \n",
    "        start = 0\n",
    "        ntest = test_X.size()[0]\n",
    "        predicted_label = torch.LongTensor(test_label.size())\n",
    "        for i in range(0, ntest, self.batch_size):\n",
    "            end = min(ntest, start+self.batch_size)\n",
    "            with torch.no_grad():\n",
    "                if self.cuda:\n",
    "#                     embed, _ = self.MapNet(test_X[start:end].cuda())\n",
    "#                     output = self.model(embed)\n",
    "                    output = self.model(test_X[start:end].cuda())\n",
    "                else:\n",
    "#                     embed, _ = self.MapNet(test_X[start:end])\n",
    "#                     output = self.model(embed)\n",
    "                    output = self.model(test_X[start:end])\n",
    "            _, predicted_label[start:end] = torch.max(output, 1)\n",
    "        \n",
    "#             print(\"-----------以下为预测标签---------\")\n",
    "#             print(predicted_label)\n",
    "#             print(\"+++++++++++以下为测试标签++++++++++\")\n",
    "#             print(test_label)\n",
    "#             print(\"===========以下为映射标签===========\")\n",
    "#             print(map_label(test_label, target_classes))\n",
    "#             print(\"???????????????????end???????????????\")\n",
    "            start = end\n",
    "\n",
    "        acc = self.compute_per_class_acc_gzsl(test_label, predicted_label, target_classes)\n",
    "        return acc\n",
    "\n",
    "    def compute_per_class_acc_gzsl(self, test_label, predicted_label, target_classes):\n",
    "        acc_per_class = 0\n",
    "        for i in target_classes:\n",
    "            idx = (test_label == i)\n",
    "#             print(\"===========以下为相同类别的idx===========\")\n",
    "#             print(target_classes)\n",
    "#             print(idx)\n",
    "#             print(\"===========以下为相同类别的idx===========\")\n",
    "            acc_per_class += float(torch.sum(test_label[idx] == predicted_label[idx])) / float(torch.sum(idx))\n",
    "        acc_per_class /= target_classes.size(0)\n",
    "        return acc_per_class \n",
    "\n",
    "    # test_label is integer \n",
    "    def val(self, test_X, test_label, target_classes):\n",
    "        start = 0\n",
    "        ntest = test_X.size()[0]\n",
    "        predicted_label = torch.LongTensor(test_label.size())\n",
    "        for i in range(0, ntest, self.batch_size):\n",
    "            end = min(ntest, start+self.batch_size)\n",
    "            with torch.no_grad():\n",
    "                if self.cuda:\n",
    "#                     embed, _ = self.MapNet(test_X[start:end].cuda())\n",
    "#                     output = self.model(embed)\n",
    "                    output = self.model(test_X[start:end].cuda())\n",
    "                else:\n",
    "#                     embed, _ = self.MapNet(test_X[start:end])\n",
    "#                     output = self.model(embed)\n",
    "                    output = self.model(test_X[start:end])\n",
    "            _, predicted_label[start:end] = torch.max(output, 1)\n",
    "#             print(\"-----------以下为预测标签---------\")\n",
    "#             print(predicted_label)\n",
    "#             print(\"+++++++++++以下为测试标签++++++++++\")\n",
    "#             print(test_label)\n",
    "            start = end\n",
    "\n",
    "        acc = self.compute_per_class_acc(test_label, predicted_label, target_classes.size(0))\n",
    "        \n",
    "        return acc\n",
    "\n",
    "    def compute_per_class_acc(self, test_label, predicted_label, nclass):\n",
    "        acc_per_class = torch.FloatTensor(nclass).fill_(0)\n",
    "        for i in range(nclass):\n",
    "            idx = (test_label == i)\n",
    "#             print(\"===========以下为相同类别的idx===========\")\n",
    "#             print(nclass)\n",
    "#             print(idx)\n",
    "#             print(\"===========以下为相同类别的idx===========\")\n",
    "            acc_per_class[i] = float(torch.sum(test_label[idx]==predicted_label[idx])) / float(torch.sum(idx))\n",
    "        return acc_per_class.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "# import util\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "import sys\n",
    "\n",
    "class LINEAR_LOGSOFTMAX(nn.Module):\n",
    "    def __init__(self, input_dim, nclass):\n",
    "        super(LINEAR_LOGSOFTMAX, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, nclass)\n",
    "        self.logic = nn.LogSoftmax(dim=1)\n",
    "    def forward(self, x): \n",
    "        o = self.logic(self.fc(x)) \n",
    "        return o \n",
    "\n",
    "class CLASSIFIER:\n",
    "    # train_Y is interger \n",
    "    def __init__(self, _train_X, _train_Y, map_net, resSize, data_loader, _nclass, _cuda, _lr=0.00001, _beta1=0.5, _nepoch=20, _batch_size=100, generalized=True):\n",
    "        self.train_X =  _train_X \n",
    "        self.train_Y = _train_Y \n",
    "        \n",
    "        self.test_seen_feature = data_loader.test_seen_feature\n",
    "        self.test_seen_label = data_loader.test_seen_label \n",
    "        \n",
    "        self.test_unseen_feature = data_loader.test_unseen_feature\n",
    "        self.test_unseen_label = data_loader.test_unseen_label \n",
    "        \n",
    "        self.seenclasses = data_loader.seenclasses\n",
    "        self.unseenclasses = data_loader.unseenclasses\n",
    "        \n",
    "        self.MapNet=map_net\n",
    "        \n",
    "        self.batch_size = _batch_size\n",
    "        self.nepoch = _nepoch\n",
    "        #分几类\n",
    "        self.nclass = _nclass\n",
    "        self.input_dim = resSize\n",
    "        self.cuda = _cuda\n",
    "        self.model =  LINEAR_LOGSOFTMAX(self.input_dim, self.nclass)\n",
    "\n",
    "        self.model.apply(weights_init)\n",
    "        self.criterion = nn.NLLLoss() \n",
    "        \n",
    "        self.input = torch.FloatTensor(_batch_size, _train_X.size(1))\n",
    "        self.label = torch.LongTensor(_batch_size) \n",
    "        \n",
    "        self.lr = _lr\n",
    "        self.beta1 = _beta1\n",
    "        # setup optimizer\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=_lr, betas=(_beta1, 0.999))\n",
    "\n",
    "        # if self.cuda:\n",
    "            # self.model.cuda()\n",
    "            # self.criterion.cuda()\n",
    "            # self.input = self.input.cuda()\n",
    "            # self.label = self.label.cuda()\n",
    "\n",
    "        self.index_in_epoch = 0\n",
    "        self.epochs_completed = 0\n",
    "        self.ntrain = self.train_X.size()[0]\n",
    "\n",
    "        \n",
    "        if generalized:\n",
    "            self.acc_seen, self.acc_unseen, self.H = self.fit()\n",
    "        else:\n",
    "            self.acc, self.label, self.preLabel = self.fit_zsl()\n",
    "    \n",
    "    \n",
    "    def fit_zsl(self):\n",
    "        best_acc = 0\n",
    "        pre_label = 0\n",
    "        tes_label = 0\n",
    "        mean_loss = 0\n",
    "        for epoch in range(self.nepoch):\n",
    "            for i in range(0, self.ntrain, self.batch_size):      \n",
    "                self.model.zero_grad()\n",
    "                batch_input, batch_label = self.next_batch(self.batch_size) \n",
    "                self.input.copy_(batch_input) \n",
    "                self.label.copy_(batch_label)\n",
    "#                 embed, _=self.MapNet(self.input)\n",
    "#                 output = self.model(embed)\n",
    "                output = self.model(self.input)\n",
    "                loss = self.criterion(output, self.label)\n",
    "                mean_loss += loss.data\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            acc,test_label,predicted_label = self.val(self.test_unseen_feature, self.test_unseen_label, self.unseenclasses)\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                tes_label = test_label\n",
    "                pre_label = predicted_label\n",
    "        print('Training classifier loss= %.4f' % (loss))\n",
    "        return best_acc, tes_label, pre_label\n",
    "\n",
    "    def fit(self):\n",
    "        best_H = 0\n",
    "        best_seen = 0\n",
    "        best_unseen = 0\n",
    "        for epoch in range(self.nepoch):\n",
    "            for i in range(0, self.ntrain, self.batch_size):      \n",
    "                self.model.zero_grad()\n",
    "                batch_input, batch_label = self.next_batch(self.batch_size) \n",
    "                self.input.copy_(batch_input)\n",
    "                self.label.copy_(batch_label)\n",
    "#                 embed, _ = self.MapNet(self.input)\n",
    "#                 output = self.model(embed)\n",
    "                output = self.model(self.input)\n",
    "                loss = self.criterion(output, self.label)\n",
    "#               mean_loss += loss.data\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            acc_seen = self.val_gzsl(self.test_seen_feature, self.test_seen_label, self.seenclasses)\n",
    "            acc_unseen = self.val_gzsl(self.test_unseen_feature, self.test_unseen_label, self.unseenclasses)\n",
    "            if (acc_seen+acc_unseen)==0:\n",
    "                print('a bug')\n",
    "                H=0\n",
    "            else:\n",
    "                H = 2*acc_seen*acc_unseen / (acc_seen+acc_unseen)\n",
    "            if H > best_H:\n",
    "                best_seen = acc_seen\n",
    "                best_unseen = acc_unseen\n",
    "                best_H = H\n",
    "        return best_seen, best_unseen, best_H\n",
    "                     \n",
    "    def next_batch(self, batch_size):\n",
    "        start = self.index_in_epoch\n",
    "        # shuffle the data at the first epoch\n",
    "        if self.epochs_completed == 0 and start == 0:\n",
    "            perm = torch.randperm(self.ntrain)\n",
    "            self.train_X = self.train_X[perm]\n",
    "            self.train_Y = self.train_Y[perm]\n",
    "        # the last batch\n",
    "        if start + batch_size > self.ntrain:\n",
    "            self.epochs_completed += 1\n",
    "            rest_num_examples = self.ntrain - start\n",
    "            if rest_num_examples > 0:\n",
    "                X_rest_part = self.train_X[start:self.ntrain]\n",
    "                Y_rest_part = self.train_Y[start:self.ntrain]\n",
    "            # shuffle the data\n",
    "            perm = torch.randperm(self.ntrain)\n",
    "            self.train_X = self.train_X[perm]\n",
    "            self.train_Y = self.train_Y[perm]\n",
    "            # start next epoch\n",
    "            start = 0\n",
    "            self.index_in_epoch = batch_size - rest_num_examples\n",
    "            end = self.index_in_epoch\n",
    "            X_new_part = self.train_X[start:end]\n",
    "            Y_new_part = self.train_Y[start:end]\n",
    "            #print(start, end)\n",
    "            if rest_num_examples > 0:\n",
    "                return torch.cat((X_rest_part, X_new_part), 0) , torch.cat((Y_rest_part, Y_new_part), 0)\n",
    "            else:\n",
    "                return X_new_part, Y_new_part\n",
    "        else:\n",
    "            self.index_in_epoch += batch_size\n",
    "            end = self.index_in_epoch\n",
    "            #print(start, end)\n",
    "            # from index start to index end-1\n",
    "            return self.train_X[start:end], self.train_Y[start:end]\n",
    "\n",
    "\n",
    "    def val_gzsl(self, test_X, test_label, target_classes): \n",
    "        start = 0\n",
    "        ntest = test_X.size()[0]\n",
    "        predicted_label = torch.LongTensor(test_label.size())\n",
    "        for i in range(0, ntest, self.batch_size):\n",
    "            end = min(ntest, start+self.batch_size)\n",
    "            with torch.no_grad():\n",
    "                if self.cuda:\n",
    "#                     embed, _ = self.MapNet(test_X[start:end].cuda())\n",
    "#                     output = self.model(embed)\n",
    "                    output = self.model(test_X[start:end].cuda())\n",
    "                else:\n",
    "#                     embed, _ = self.MapNet(test_X[start:end])\n",
    "#                     output = self.model(embed)\n",
    "                    output = self.model(test_X[start:end])\n",
    "            _, predicted_label[start:end] = torch.max(output, 1)\n",
    "        \n",
    "#             print(\"-----------以下为预测标签---------\")\n",
    "#             print(predicted_label)\n",
    "#             print(\"+++++++++++以下为测试标签++++++++++\")\n",
    "#             print(test_label)\n",
    "#             print(\"===========以下为映射标签===========\")\n",
    "#             print(map_label(test_label, target_classes))\n",
    "#             print(\"???????????????????end???????????????\")\n",
    "            start = end\n",
    "\n",
    "        acc = self.compute_per_class_acc_gzsl(test_label, predicted_label, target_classes)\n",
    "        return acc\n",
    "\n",
    "    def compute_per_class_acc_gzsl(self, test_label, predicted_label, target_classes):\n",
    "        acc_per_class = 0\n",
    "        for i in target_classes:\n",
    "            idx = (test_label == i)\n",
    "#             print(\"===========以下为相同类别的idx===========\")\n",
    "#             print(target_classes)\n",
    "#             print(idx)\n",
    "#             print(\"===========以下为相同类别的idx===========\")\n",
    "            acc_per_class += float(torch.sum(test_label[idx] == predicted_label[idx])) / float(torch.sum(idx))\n",
    "        acc_per_class /= target_classes.size(0)\n",
    "        return acc_per_class \n",
    "\n",
    "    # test_label is integer \n",
    "    def val(self, test_X, test_label, target_classes): \n",
    "        start = 0\n",
    "        ntest = test_X.size()[0]\n",
    "        predicted_label = torch.LongTensor(test_label.size())\n",
    "        for i in range(0, ntest, self.batch_size):\n",
    "            end = min(ntest, start+self.batch_size)\n",
    "            with torch.no_grad():\n",
    "                if self.cuda:\n",
    "#                     embed, _ = self.MapNet(test_X[start:end].cuda())\n",
    "#                     output = self.model(embed)\n",
    "                    output = self.model(test_X[start:end].cuda())\n",
    "                else:\n",
    "#                     embed, _ = self.MapNet(test_X[start:end])\n",
    "#                     output = self.model(embed)\n",
    "                    output = self.model(test_X[start:end])\n",
    "            _, predicted_label[start:end] = torch.max(output, 1)\n",
    "#             print(\"-----------以下为预测标签---------\")\n",
    "#             print(predicted_label)\n",
    "#             print(\"+++++++++++以下为测试标签++++++++++\")\n",
    "#             print(test_label)\n",
    "            start = end\n",
    "\n",
    "        acc = self.compute_per_class_acc(test_label, predicted_label, target_classes.size(0))\n",
    "        \n",
    "        return acc,test_label,predicted_label\n",
    "\n",
    "    def compute_per_class_acc(self, test_label, predicted_label, nclass):\n",
    "        acc_per_class = torch.FloatTensor(nclass).fill_(0)\n",
    "        for i in range(nclass):\n",
    "            idx = (test_label == i)\n",
    "#             print(\"===========以下为相同类别的idx===========\")\n",
    "#             print(nclass)\n",
    "#             print(idx)\n",
    "#             print(\"===========以下为相同类别的idx===========\")\n",
    "            acc_per_class[i] = float(torch.sum(test_label[idx]==predicted_label[idx])) / float(torch.sum(idx))\n",
    "        return acc_per_class.mean()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "def reparameter(mu,sigma):\n",
    "    return (torch.randn_like(mu) *sigma) + mu\n",
    "\n",
    "class Embedding_Net(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(Embedding_Net, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(opt.attSize + opt.nz, opt.embedSize) \n",
    "        self.fc2 = nn.Linear(opt.embedSize, opt.resSize)\n",
    "        self.lrelu = nn.LeakyReLU(0.2, True)\n",
    "        self.relu = nn.ReLU(True)\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, noise, att):\n",
    "        h = torch.cat((noise, att), 1)\n",
    "        embedding= F.normalize(self.relu(self.fc1(h)), dim=1)\n",
    "        out_z = self.fc2(embedding)\n",
    "        return embedding,out_z\n",
    "\n",
    "class MLP_G(nn.Module): \n",
    "    def __init__(self, opt):\n",
    "        super(MLP_G, self).__init__()\n",
    "        self.fc1 = nn.Linear(opt.embedSize + opt.nz, opt.ngh)\n",
    "        self.fc2 = nn.Linear(opt.ngh, opt.resSize)\n",
    "        self.relu = nn.ReLU(True)\n",
    "\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, noise, att):\n",
    "        h = torch.cat((noise, att), 1)\n",
    "        ngh = F.normalize(self.fc1(h), dim=1)\n",
    "        h = self.relu(self.fc2(ngh))\n",
    "        return ngh,h\n",
    "\n",
    "class MLP_CRITIC(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(MLP_CRITIC, self).__init__()\n",
    "        self.fc1 = nn.Linear(opt.resSize + opt.embedSize, opt.ndh)\n",
    "        self.fc2 = nn.Linear(opt.ndh, 1)\n",
    "        self.lrelu = nn.LeakyReLU(0.2, True)\n",
    "#         self.relu = nn.ReLU(True)\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, x, att):\n",
    "        h = torch.cat((x, att), 1)\n",
    "#         h = self.lrelu(self.fc1(h))\n",
    "#         ndh = self.lrelu(self.fc1(h))\n",
    "        ndh = F.normalize(self.lrelu(self.fc1(h)), dim=1)\n",
    "        h = self.fc2(ndh)\n",
    "        return ndh,h\n",
    "\n",
    "class Dis_Embed_Att(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(Dis_Embed_Att, self).__init__()\n",
    "        self.fc1 = nn.Linear(opt.ndh+opt.attSize, opt.nhF)\n",
    "        #self.fc2 = nn.Linear(opt.ndh, opt.ndh)\n",
    "        self.fc2 = nn.Linear(opt.nhF, 1)\n",
    "        self.lrelu = nn.LeakyReLU(0.2, True)\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, input):\n",
    "        h = self.lrelu(self.fc1(input))\n",
    "        h = self.fc2(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(dataroot='./datas', syn_num=100, gzsl=False, preprocessing=True, standardization=False, validation=False, workers=2, batch_size=40, resSize=512, attSize=28, nz=2, embedSize=28, outzSize=256, ngh=256, ndh=256, nhF=128, ins_weight=0.001, cls_weight=0.001, ins_temp=0.1, cls_temp=0.1, nepoch=223, critic_iter=5, lr=9e-06, lr_decay_epoch=100, lr_dec_rate=0.99, lambda1=10, classifier_lr=0.0001, beta1=0.5, cuda=False, manualSeed=3483, nclass_all=8, nclass_seen=4, gpus='0')\n",
      "Random Seed:  3483\n",
      "tensor([0, 1, 2, 3])\n",
      "tensor([0, 1, 2, 3])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
      "# of training samples:  8000\n",
      "[0/223] Loss_D: 1.3814 Loss_G: 0.0442, Wasserstein_dist: 0.1826, real_ins_contras_loss:0.2544, fake_ins_contras_loss:0.6651, cls_loss_real: 0.5950, cls_loss_fake: 1.3613, contras_loss: 0.5248\n",
      "Training classifier_train loss= 1.3338\n",
      "Training classifier loss= 1.3709\n",
      "seen class accuracy=0.2500, unseen class accuracy=0.3970 \n",
      "-------------------test_label----------------------\n",
      "tensor([0, 0, 0,  ..., 3, 3, 3])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])\n",
      "-------------------pre_label----------------------\n",
      "tensor([0, 0, 0,  ..., 0, 2, 2])\n",
      "tensor([0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 2, 2, 0, 0, 2, 0, 2, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "tensor([2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2])\n",
      "[1/223] Loss_D: 0.9575 Loss_G: 0.0695, Wasserstein_dist: 0.2533, real_ins_contras_loss:0.2444, fake_ins_contras_loss:0.6831, cls_loss_real: 0.3261, cls_loss_fake: 1.2930, contras_loss: 0.4608\n",
      "Training classifier_train loss= 1.3538\n",
      "Training classifier loss= 1.3757\n",
      "seen class accuracy=0.2500, unseen class accuracy=0.2500 \n",
      "-------------------test_label----------------------\n",
      "tensor([0, 0, 0,  ..., 3, 3, 3])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])\n",
      "-------------------pre_label----------------------\n",
      "tensor([2, 2, 2,  ..., 2, 2, 2])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "[2/223] Loss_D: 0.6313 Loss_G: 0.1039, Wasserstein_dist: 0.3886, real_ins_contras_loss:0.2362, fake_ins_contras_loss:0.7273, cls_loss_real: 0.1700, cls_loss_fake: 1.4116, contras_loss: 0.3771\n",
      "Training classifier_train loss= 1.3613\n",
      "Training classifier loss= 1.3801\n",
      "seen class accuracy=0.2555, unseen class accuracy=0.2211 \n",
      "-------------------test_label----------------------\n",
      "tensor([0, 0, 0,  ..., 3, 3, 3])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])\n",
      "-------------------pre_label----------------------\n",
      "tensor([2, 2, 2,  ..., 2, 0, 0])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "tensor([2, 0, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 2, 0, 0, 0, 2])\n",
      "tensor([0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 2, 0, 2, 2, 0, 2])\n",
      "[3/223] Loss_D: 0.3457 Loss_G: 0.1875, Wasserstein_dist: 0.5541, real_ins_contras_loss:0.2370, fake_ins_contras_loss:0.7904, cls_loss_real: 0.1141, cls_loss_fake: 1.5156, contras_loss: 0.3511\n",
      "Training classifier_train loss= 1.3676\n",
      "Training classifier loss= 1.3822\n",
      "seen class accuracy=0.2498, unseen class accuracy=0.4201 \n",
      "-------------------test_label----------------------\n",
      "tensor([0, 0, 0,  ..., 3, 3, 3])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])\n",
      "-------------------pre_label----------------------\n",
      "tensor([0, 0, 0,  ..., 1, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])\n",
      "[4/223] Loss_D: 0.0181 Loss_G: 0.3287, Wasserstein_dist: 0.7719, real_ins_contras_loss:0.2320, fake_ins_contras_loss:0.8714, cls_loss_real: 0.0716, cls_loss_fake: 1.5670, contras_loss: 0.3666\n",
      "Training classifier_train loss= 1.3693\n",
      "Training classifier loss= 1.3806\n",
      "seen class accuracy=0.2498, unseen class accuracy=0.2499 \n",
      "-------------------test_label----------------------\n",
      "tensor([0, 0, 0,  ..., 3, 3, 3])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])\n",
      "-------------------pre_label----------------------\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "[5/223] Loss_D: -0.2344 Loss_G: 0.4722, Wasserstein_dist: 1.0504, real_ins_contras_loss:0.2386, fake_ins_contras_loss:0.9454, cls_loss_real: 0.0522, cls_loss_fake: 2.4913, contras_loss: 0.3494\n",
      "Training classifier_train loss= 1.3679\n",
      "Training classifier loss= 1.3819\n",
      "seen class accuracy=0.5000, unseen class accuracy=0.2524 \n",
      "-------------------test_label----------------------\n",
      "tensor([0, 0, 0,  ..., 3, 3, 3])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])\n",
      "-------------------pre_label----------------------\n",
      "tensor([2, 2, 1,  ..., 1, 1, 1])\n",
      "tensor([2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 1, 2, 3, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1])\n",
      "[6/223] Loss_D: -0.6219 Loss_G: 0.6440, Wasserstein_dist: 1.2957, real_ins_contras_loss:0.2315, fake_ins_contras_loss:0.9935, cls_loss_real: 0.0272, cls_loss_fake: 2.5022, contras_loss: 0.3269\n",
      "Training classifier_train loss= 1.3679\n",
      "Training classifier loss= 1.3814\n",
      "seen class accuracy=0.5000, unseen class accuracy=0.2500 \n",
      "-------------------test_label----------------------\n",
      "tensor([0, 0, 0,  ..., 3, 3, 3])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])\n",
      "-------------------pre_label----------------------\n",
      "tensor([1, 1, 1,  ..., 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "[7/223] Loss_D: -0.9809 Loss_G: 0.8354, Wasserstein_dist: 1.6460, real_ins_contras_loss:0.2426, fake_ins_contras_loss:1.0594, cls_loss_real: 0.0206, cls_loss_fake: 4.3478, contras_loss: 0.3256\n",
      "Training classifier_train loss= 1.3645\n",
      "Training classifier loss= 1.3824\n",
      "seen class accuracy=0.0000, unseen class accuracy=0.2500 \n",
      "-------------------test_label----------------------\n",
      "tensor([0, 0, 0,  ..., 3, 3, 3])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])\n",
      "-------------------pre_label----------------------\n",
      "tensor([2, 2, 2,  ..., 2, 2, 2])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "tensor([2, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "# import classifier_embed_contras\n",
    "import torch.nn.functional as F\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--dataroot', default='./datas', help='path to dataset')\n",
    "parser.add_argument('--syn_num', type=int, default=100, help='number features to generate per class')\n",
    "parser.add_argument('--gzsl', type=bool, default=False, help='enable generalized zero-shot learning')\n",
    "parser.add_argument('--preprocessing', type=bool, default=True, help='enbale MinMaxScaler on visual features')\n",
    "parser.add_argument('--standardization', action='store_true', default=False)\n",
    "parser.add_argument('--validation', action='store_true', default=False, help='enable cross validation mode')\n",
    "parser.add_argument('--workers', type=int, help='number of data loading workers', default=2)\n",
    "# parser.add_argument('--batch_size', type=int, default=2048, help='input batch size')\n",
    "parser.add_argument('--batch_size', type=int, default=40, help='input batch size')\n",
    "parser.add_argument('--resSize', type=int, default=512, help='size of visual features')\n",
    "# parser.add_argument('--attSize', type=int, default=156 , help='size of semantic features')\n",
    "parser.add_argument('--attSize', type=int, default=28 , help='size of semantic features')\n",
    "parser.add_argument('--nz', type=int, default=2, help='noise for generation')\n",
    "parser.add_argument('--embedSize', type=int, default=28, help='size of embedding h')\n",
    "parser.add_argument('--outzSize', type=int, default=256, help='size of non-liner projection z')\n",
    "\n",
    "## network architechure\n",
    "parser.add_argument('--ngh', type=int, default=256, help='size of the hidden units in generator G')\n",
    "parser.add_argument('--ndh', type=int, default=256, help='size of the hidden units in discriminator D')\n",
    "parser.add_argument('--nhF', type=int, default=128, help='size of the hidden units comparator network F')\n",
    "\n",
    "parser.add_argument('--ins_weight', type=float, default=0.001, help='weight of the classification loss when learning G')\n",
    "parser.add_argument('--cls_weight', type=float, default=0.001, help='weight of the score function when learning G')\n",
    "parser.add_argument('--ins_temp', type=float, default=0.1, help='temperature in instance-level supervision')\n",
    "parser.add_argument('--cls_temp', type=float, default=0.1, help='temperature in class-level supervision')\n",
    "\n",
    "parser.add_argument('--nepoch', type=int, default=223, help='number of epochs to train for')\n",
    "parser.add_argument('--critic_iter', type=int, default=5, help='critic iteration, following WGAN-GP')\n",
    "# parser.add_argument('--lr', type=float, default=0.000007, help='learning rate to training')\n",
    "parser.add_argument('--lr', type=float, default=0.000009, help='learning rate to training')\n",
    "parser.add_argument('--lr_decay_epoch', type=int, default=100, help='conduct learning rate decay after every 100 epochs')\n",
    "parser.add_argument('--lr_dec_rate', type=float, default=0.99, help='learning rate decay rate')\n",
    "parser.add_argument('--lambda1', type=float, default=10, help='gradient penalty regularizer, following WGAN-GP')\n",
    "parser.add_argument('--classifier_lr', type=float, default=0.0001, help='learning rate to train softmax classifier')\n",
    "parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for adam. default=0.5')\n",
    "# parser.add_argument('--cuda', action='store_true', default=True, help='enables cuda')\n",
    "parser.add_argument('--cuda', action='store_true', default=False, help='enables cuda')\n",
    "parser.add_argument('--manualSeed', type=int, default=3483, help='manual seed')\n",
    "parser.add_argument('--nclass_all', type=int, default=8, help='number of all classes')\n",
    "parser.add_argument('--nclass_seen', type=int, default=4, help='number of all classes')\n",
    "\n",
    "parser.add_argument('--gpus', default='0', help='the number of the GPU to use')\n",
    "# opt = parser.parse_args()\n",
    "opt = parser.parse_known_args()[0]\n",
    "\n",
    "print(opt)\n",
    "\n",
    "G_loss_list = []\n",
    "D_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = opt.gpus\n",
    "\n",
    "if opt.manualSeed is None:\n",
    "    opt.manualSeed = random.randint(1, 10000)\n",
    "print(\"Random Seed: \", opt.manualSeed)\n",
    "random.seed(opt.manualSeed)\n",
    "torch.manual_seed(opt.manualSeed)\n",
    "if opt.cuda:\n",
    "    torch.cuda.manual_seed_all(opt.manualSeed)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "if torch.cuda.is_available() and not opt.cuda:\n",
    "    print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "\n",
    "# load data\n",
    "# data = util.DATA_LOADER(opt)\n",
    "data = DATA_LOADER(opt)\n",
    "print(\"# of training samples: \", data.ntrain)\n",
    "\n",
    "\n",
    "netG = MLP_G(opt)\n",
    "netMap = Embedding_Net(opt)\n",
    "netD = MLP_CRITIC(opt)\n",
    "F_ha = Dis_Embed_Att(opt)\n",
    "\n",
    "model_path = './models/'\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "contras_criterion = SupConLoss_clear(opt.ins_temp)\n",
    "mse_criterion = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "input_res = torch.FloatTensor(opt.batch_size, opt.resSize)\n",
    "input_att = torch.FloatTensor(opt.batch_size, opt.attSize)\n",
    "noise_gen = torch.FloatTensor(opt.batch_size, opt.nz)\n",
    "input_label = torch.LongTensor(opt.batch_size)\n",
    "\n",
    "def sample():\n",
    "    batch_feature, batch_label, batch_att = data.next_batch(opt.batch_size)\n",
    "    input_res.copy_(batch_feature)\n",
    "    input_att.copy_(batch_att)\n",
    "    input_label.copy_(batch_label)\n",
    "\n",
    "\n",
    "def generate_syn_test_feature(netG, map_net, classes, attribute, num):\n",
    "    nclass = classes.size(0)\n",
    "    #init tensor\n",
    "    syn_feature = torch.FloatTensor(nclass * num, opt.resSize)\n",
    "    syn_label = torch.LongTensor(nclass * num)\n",
    "    syn_att = torch.FloatTensor(num, opt.attSize)\n",
    "    syn_noise = torch.FloatTensor(num, opt.nz)\n",
    "#     if opt.cuda:\n",
    "#         syn_att = syn_att.cuda()\n",
    "#         syn_noise = syn_noise.cuda()\n",
    "\n",
    "    for i in range(nclass):\n",
    "        iclass = classes[i]\n",
    "        iclass_att = attribute[iclass+4] \n",
    "#         print(\"============================\")\n",
    "#         print(iclass_att)\n",
    "#         print(\"============================\")\n",
    "        syn_att.copy_(iclass_att.repeat(num, 1))\n",
    "        syn_noise.normal_(0, 1)\n",
    "        with torch.no_grad():\n",
    "            embed_att,_ = map_net(syn_noise,syn_att)\n",
    "            _,output = netG(syn_noise, embed_att)\n",
    "        syn_feature.narrow(0, i * num, num).copy_(output.data.cpu())\n",
    "        syn_label.narrow(0, i * num, num).fill_(iclass)\n",
    "    return syn_feature, syn_label\n",
    "\n",
    "def generate_syn_train_feature(netG, map_net, classes, attribute, num):\n",
    "    nclass = classes.size(0)\n",
    "    syn_feature = torch.FloatTensor(nclass * num, opt.resSize)\n",
    "    syn_label = torch.LongTensor(nclass * num)\n",
    "    syn_att = torch.FloatTensor(num, opt.attSize)\n",
    "    syn_noise = torch.FloatTensor(num, opt.nz)\n",
    "#     if opt.cuda:\n",
    "#         syn_att = syn_att.cuda()\n",
    "#         syn_noise = syn_noise.cuda()\n",
    "\n",
    "    for i in range(nclass):\n",
    "        iclass = classes[i]\n",
    "        iclass_att = attribute[iclass]\n",
    "#         print(\"---------------------------\")\n",
    "#         print(iclass_att)\n",
    "#         print(\"---------------------------\")\n",
    "        syn_att.copy_(iclass_att.repeat(num, 1))\n",
    "        syn_noise.normal_(0, 1)\n",
    "        with torch.no_grad():\n",
    "            embed_att,_ = map_net(syn_noise,syn_att)\n",
    "            _,output = netG(syn_noise, embed_att)\n",
    "        syn_feature.narrow(0, i * num, num).copy_(output.data.cpu())\n",
    "        syn_label.narrow(0, i * num, num).fill_(iclass)\n",
    "    return syn_feature, syn_label\n",
    "\n",
    "\n",
    "# setup optimizer\n",
    "import itertools\n",
    "optimizerD = optim.Adam(itertools.chain(netD.parameters(), netMap.parameters(), F_ha.parameters()), lr=opt.lr,\n",
    "                        betas=(opt.beta1, 0.999))\n",
    "# optimizerD = optim.Adam(netD.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "\n",
    "def calc_gradient_penalty(netD, real_data, fake_data, input_att):\n",
    "    # print real_data.size()\n",
    "    alpha = torch.rand(opt.batch_size, 1)\n",
    "    alpha = alpha.expand(real_data.size())\n",
    "#     if opt.cuda:\n",
    "#         alpha = alpha.cuda()\n",
    "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "#     if opt.cuda:\n",
    "#         interpolates = interpolates.cuda()\n",
    "    interpolates = Variable(interpolates, requires_grad=True)\n",
    "    _,disc_interpolates = netD(interpolates, input_att)\n",
    "    ones = torch.ones(disc_interpolates.size())\n",
    "#     if opt.cuda:\n",
    "#         ones = ones.cuda()\n",
    "    gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
    "                              grad_outputs=ones,\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * opt.lambda1\n",
    "    return gradient_penalty\n",
    "\n",
    "# use the for-loop to save the GPU-memory\n",
    "def class_scores_for_loop(embed, input_label, relation_net):\n",
    "    #all_scores=torch.FloatTensor(embed.shape[0],opt.nclass_seen).cuda()\n",
    "    all_scores=torch.FloatTensor(embed.shape[0],opt.nclass_seen)\n",
    "    for i, i_embed in enumerate(embed):\n",
    "        expand_embed = i_embed.repeat(opt.nclass_seen, 1)#.reshape(embed.shape[0] * opt.nclass_seen, -1)\n",
    "        #all_scores[i]=(torch.div(relation_net(torch.cat((expand_embed, data.attribute_seen.cuda()), dim=1)),opt.cls_temp).squeeze())\n",
    "        all_scores[i]=(torch.div(relation_net(torch.cat((expand_embed, data.attribute_seen), dim=1)),opt.cls_temp).squeeze())\n",
    "    score_max, _ = torch.max(all_scores, dim=1, keepdim=True)\n",
    "    # normalize the scores for stable training\n",
    "    scores_norm = all_scores - score_max.detach()\n",
    "    #mask = F.one_hot(input_label, num_classes=opt.nclass_seen).float().cuda()\n",
    "    mask = F.one_hot(input_label, num_classes=opt.nclass_seen).float()\n",
    "    exp_scores = torch.exp(scores_norm)\n",
    "    log_scores = scores_norm - torch.log(exp_scores.sum(1, keepdim=True))\n",
    "    cls_loss = -((mask * log_scores).sum(1) / mask.sum(1)).mean()\n",
    "    return cls_loss\n",
    "\n",
    "# It is much faster to use the matrix, but it cost much GPU memory.\n",
    "def class_scores_in_matrix(embed, input_label, relation_net):\n",
    "    expand_embed = embed.unsqueeze(dim=1).repeat(1, opt.nclass_seen, 1).reshape(embed.shape[0] * opt.nclass_seen, -1)\n",
    "    expand_att = data.attribute_seen.unsqueeze(dim=0).repeat(embed.shape[0], 1, 1).reshape(\n",
    "        embed.shape[0] * opt.nclass_seen, -1).cuda()\n",
    "    all_scores = torch.div(relation_net(torch.cat((expand_embed, expand_att), dim=1)),opt.cls_temp).reshape(embed.shape[0],\n",
    "                                                                                                    opt.nclass_seen)\n",
    "    score_max, _ = torch.max(all_scores, dim=1, keepdim=True)\n",
    "    scores_norm = all_scores - score_max.detach()\n",
    "    #mask = F.one_hot(input_label, num_classes=opt.nclass_seen).float().cuda()\n",
    "    mask = F.one_hot(input_label, num_classes=opt.nclass_seen).float()\n",
    "    exp_scores = torch.exp(scores_norm)\n",
    "    log_scores = scores_norm - torch.log(exp_scores.sum(1, keepdim=True))\n",
    "    cls_loss = -((mask * log_scores).sum(1) / mask.sum(1)).mean()\n",
    "    return cls_loss\n",
    "\n",
    "\n",
    "for epoch in range(opt.nepoch):\n",
    "    FP = 0\n",
    "    mean_lossD = 0\n",
    "    mean_lossG = 0\n",
    "    for i in range(0, data.ntrain, opt.batch_size):\n",
    "#     for i in range(0, 4* opt.batch_size, opt.batch_size):\n",
    "#     for i in range(0, 4, opt.batch_size):\n",
    "        ############################\n",
    "        # (1) Update D network: optimize WGAN-GP objective, Equation (2)\n",
    "        ###########################\n",
    "        for p in netD.parameters():  # reset requires_grad\n",
    "            p.requires_grad = True  # they are set to False below in netG update\n",
    "        for p in netMap.parameters():  # reset requires_grad\n",
    "            p.requires_grad = True\n",
    "        for p in F_ha.parameters():  # reset requires_grad\n",
    "            p.requires_grad = True\n",
    "\n",
    "        for iter_d in range(opt.critic_iter):\n",
    "            sample()\n",
    "            netD.zero_grad()\n",
    "            netMap.zero_grad()\n",
    "            #\n",
    "            # train with realG\n",
    "            # sample a mini-batch\n",
    "            noise_gen.normal_(0, 1)\n",
    "            embed_att, out_res = netMap(noise_gen, input_att)\n",
    "            embed_fea,criticD_real = netD(input_res, embed_att)\n",
    "            criticD_real = criticD_real.mean()\n",
    "            \n",
    "            real_ins_contras_loss = contras_criterion(embed_fea, input_label)\n",
    "            real_att_contras_loss = contras_criterion(embed_att, input_label)\n",
    "#             print(\"========================================\")\n",
    "#             print(real_ins_contras_loss)\n",
    "#             print(\"========================================\")\n",
    "\n",
    "            # train with fakeG\n",
    "            noise_gen.normal_(0, 1)\n",
    "            embed_att, out_res = netMap(noise_gen,input_att)\n",
    "            _,fake = netG(noise_gen, embed_att)\n",
    "            _,criticD_fake = netD(fake.detach(), embed_att)\n",
    "            criticD_fake = criticD_fake.mean()\n",
    "            \n",
    "            # gradient penalty\n",
    "            gradient_penalty = calc_gradient_penalty(netD, input_res, fake.data, embed_att)\n",
    "            Wasserstein_D = criticD_real - criticD_fake\n",
    "            res_L = mse_criterion(input_res,out_res)\n",
    "\n",
    "            cls_loss_real = class_scores_for_loop(embed_fea, input_label, F_ha)\n",
    "\n",
    "            # D_cost = criticD_fake - criticD_real + gradient_penalty + real_ins_contras_loss + cls_loss_real + input_res.mean()- outz_real.mean()\n",
    "            D_cost = criticD_fake - criticD_real + gradient_penalty + cls_loss_real + real_ins_contras_loss + real_att_contras_loss + res_L\n",
    "\n",
    "            D_cost.backward()\n",
    "            optimizerD.step()\n",
    "        ############################\n",
    "        # (2) Update G network: optimize WGAN-GP objective, Equation (2)\n",
    "        ###########################\n",
    "        for p in netD.parameters():  # reset requires_grad\n",
    "            p.requires_grad = False  # avoid computation\n",
    "        for p in netMap.parameters():  # reset requires_grad\n",
    "            p.requires_grad = False\n",
    "        for p in F_ha.parameters():  # reset requires_grad\n",
    "            p.requires_grad = False\n",
    "\n",
    "        netG.zero_grad()\n",
    "        noise_gen.normal_(0, 1)\n",
    "\n",
    "        embed_att, out_res = netMap(noise_gen, input_att)\n",
    "        embed_G_fake,fake = netG(noise_gen, embed_att)\n",
    "        \n",
    "        embed_fea1,criticG_real = netD(input_res, embed_att)\n",
    "        embed_fea2,criticG_fake = netD(fake, embed_att)\n",
    "        criticG_fake = criticG_fake.mean()\n",
    "        G_cost = -criticG_fake\n",
    "        \n",
    "        contras_loss = contras_criterion(embed_G_fake, input_label)\n",
    "\n",
    "        all_outz = torch.cat((embed_fea2, embed_fea1.detach()), dim=0)\n",
    "        fake_ins_contras_loss = contras_criterion(all_outz, torch.cat((input_label, input_label), dim=0))\n",
    "        cls_loss_fake = class_scores_for_loop(embed_fea2, input_label, F_ha)\n",
    "\n",
    "#         print(\"+++++++++++++++++++++++++++++++++\")\n",
    "#         print(G_cost)\n",
    "#         print(contras_loss)\n",
    "#         print(fake_ins_contras_loss)\n",
    "#         print(cls_loss_fake)\n",
    "#         print(\"+++++++++++++++++++++++++++++++++\")\n",
    "\n",
    "\n",
    "        errG = G_cost + contras_loss + opt.ins_weight * fake_ins_contras_loss + opt.cls_weight * cls_loss_fake  # + opt.ins_weight * c_errG\n",
    "#         errG = G_cost\n",
    "        errG.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "    F_ha.zero_grad()\n",
    "\n",
    "    if (epoch + 1) % opt.lr_decay_epoch == 0:\n",
    "        for param_group in optimizerD.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * opt.lr_dec_rate\n",
    "        for param_group in optimizerG.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * opt.lr_dec_rate\n",
    "\n",
    "    mean_lossG /= data.ntrain / opt.batch_size\n",
    "    mean_lossD /= data.ntrain / opt.batch_size\n",
    "    print('[%d/%d] Loss_D: %.4f Loss_G: %.4f, Wasserstein_dist: %.4f, real_ins_contras_loss:%.4f, fake_ins_contras_loss:%.4f, cls_loss_real: %.4f, cls_loss_fake: %.4f, contras_loss: %.4f'% (epoch, opt.nepoch, D_cost, G_cost, Wasserstein_D, real_ins_contras_loss, fake_ins_contras_loss, cls_loss_real, cls_loss_fake, contras_loss))\n",
    "    # print('[%d/%d] Loss_D: %.4f Loss_G: %.4f, Wasserstein_dist: %.4f, cls_loss_real: %.4f'% (epoch, opt.nepoch, D_cost, G_cost, Wasserstein_D, cls_loss_real))\n",
    "    \n",
    "# evaluate the model, set G to evaluation mode\n",
    "    netG.eval()\n",
    "\n",
    "    for p in netMap.parameters():  # reset requires_grad\n",
    "        p.requires_grad = False\n",
    "\n",
    "    if opt.gzsl: # Generalized zero-shot learning\n",
    "        syn_feature, syn_label = generate_syn_feature(netG, data.unseenclasses, data.attribute, opt.syn_num)\n",
    "\n",
    "        train_X = torch.cat((data.train_feature, syn_feature), 0)\n",
    "        train_Y = torch.cat((data.train_label, syn_label), 0)\n",
    "\n",
    "        nclass = opt.nclass_all\n",
    "\n",
    "        cls = CLASSIFIER(train_X, train_Y, netMap, opt.resSize, data, nclass, opt.cuda, opt.classifier_lr, 0.5, 25, 80, True)\n",
    "        print('unseen=%.4f, seen=%.4f, h=%.4f' % (cls.acc_unseen, cls.acc_seen, cls.H))\n",
    "\n",
    "    else:  # conventional zero-shot learning\n",
    "        syn_test_feature, syn_test_label = generate_syn_test_feature(netG, netMap, data.unseenclasses, data.attribute, opt.syn_num) \n",
    "        syn_train_feature, syn_train_label = generate_syn_train_feature(netG, netMap, data.seenclasses, data.attribute, opt.syn_num)\n",
    "\n",
    "        cls_seen = CLASSIFIER_TRAIN(syn_train_feature, syn_train_label, netMap,opt.resSize, data,data.seenclasses.size(0), opt.cuda, opt.classifier_lr, 0.5, 100,80,False)\n",
    "        cls_unseen = CLASSIFIER(syn_test_feature, syn_test_label, netMap,opt.resSize, data,data.unseenclasses.size(0), opt.cuda, opt.classifier_lr, 0.5, 100,80,False)\n",
    "        \n",
    "        seen_acc = cls_seen.acc\n",
    "        unseen_acc = cls_unseen.acc\n",
    "        test_label = cls_unseen.label\n",
    "        pre_label = cls_unseen.preLabel\n",
    "        \n",
    "        print('seen class accuracy=%.4f, unseen class accuracy=%.4f '%(seen_acc, unseen_acc))\n",
    "        \n",
    "        if unseen_acc > 0 :\n",
    "            print(\"-------------------test_label----------------------\")\n",
    "            print(test_label)\n",
    "            print(test_label[0:20])\n",
    "            print(test_label[2000:2020])\n",
    "            print(test_label[4000:4020])\n",
    "            print(test_label[6000:6020])\n",
    "            print(\"-------------------pre_label----------------------\")\n",
    "            print(pre_label)\n",
    "            print(pre_label[0:20])\n",
    "            print(pre_label[2000:2020])\n",
    "            print(pre_label[4000:4020])\n",
    "            print(pre_label[6000:6020])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:kh] *",
   "language": "python",
   "name": "conda-env-kh-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
